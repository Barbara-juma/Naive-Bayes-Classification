# -*- coding: utf-8 -*-
"""K Nearest Neighbor Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mZzCjVrEQngzpBJNtx806Di-bDqDYR6U

#Importing Libraries
"""

import pandas as pd #data processing
import numpy as np #linear algebra
import matplotlib.pyplot as plt #plots

"""#Loading the dataset

1.Training Dataset
"""

Train = pd.read_csv('/content/train (5).csv')
Train.head()

Train.tail()

"""2.Testing Dataset"""

Test = pd.read_csv('/content/test (1).csv')
Test.head()

Test.tail()

"""#Accessing information from the training dataset"""

#checking for info
Train.info()

#checking for the number of rows and columns
Train.shape

#checking for data types
Train.dtypes

#seeing if we have duplicated values
Train.duplicated().sum()

#checking if we have missing values
Train.isnull().sum()

#Checking for Anomalies/outliers
Q1 = Train.quantile(0.25)
Q3 = Train.quantile(0.75)
IQR = Q3 - Q1

((Train < (Q1 - 1.5 * IQR)) | (Train > (Q3 + 1.5 * IQR))).sum()

"""* The dataset does not have duplicated values
* The dataset contains missing values
* The dataset has outliers

#Accessing information from the test dataset
"""

#checking for info
Test.info()

#checking for the number of rows and columns
Test.shape

#data types
Test.dtypes

#Checking if we have duplicated values
Test.duplicated().sum()

#checking if we have missing values
Test.isnull().sum()

#Checking for Anomalies/outliers
Q1 = Test.quantile(0.25)
Q3 = Test.quantile(0.75)
IQR = Q3 - Q1

((Test < (Q1 - 1.5 * IQR)) | (Test > (Q3 + 1.5 * IQR))).sum()

"""#Data Cleaning(Training Dataset)"""

# filling null values in the age column with the age mean
Train['Age'] = Train['Age'].fillna(value = Train['Age'].mean())

Train.isnull().sum()

#filling null values in the cabin column with mode
Train['Cabin'].fillna(Train['Cabin'].mode()[0], inplace = True)

Train.isnull().sum()

#dropping null values in the embarked column since they are not many
Train.dropna(subset = ['Embarked'], inplace=True)

Train.isnull().sum()

#Correcting Outliers, filtering out the outliers by keeping only valid values
Train = Train[~((Train < (Q1 - 1.5 * IQR)) |(Train > (Q3 + 1.5 * IQR))).any(axis=1)]
Train.shape

#Dropping irrelevant columns which contains no usefull information
Train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1, inplace=True)

Train.head()

"""#Data Cleaning(Test Dataset)"""

# filling null values in the age column with the age mean
Test['Age'] = Test['Age'].fillna(value = Test['Age'].mean())

#filling null values in the cabin column with mode
Test['Cabin'].fillna(Test['Cabin'].mode()[0], inplace = True)

Test.isnull().sum()

#Correcting Outliers, filtering out the outliers by keeping only valid values
Test = Test[~((Test < (Q1 - 1.5 * IQR)) |(Test > (Q3 + 1.5 * IQR))).any(axis=1)]
Test.shape

Test.head()

"""#EDA(Test Dataset)"""

Test.drop(['PassengerId','Name','Cabin','Ticket'], axis = 1, inplace=True)

"""Encoding Categorical data"""

#Performing feature engineering on our two categorical columns
Test['Sex'] = Test['Sex'].apply(lambda x:1 if x == 'female' else 0)

Test['Embarked'] = Test['Embarked'].map({'S':1,'Q':2,'C':3})

Test.head()

Test.dtypes

"""#Univariate Analysis(Train Dataset)"""

#counts of survivors and non survivors
Train['Survived'].value_counts()

#Visualising count of survivors
import warnings
warnings.simplefilter(action = 'ignore', category = FutureWarning)
import seaborn as sns
sns.set_style('whitegrid')
sns.countplot(Train['Survived'])

sns.countplot(Train['Pclass'])

sns.countplot(Train['Sex'])

sns.countplot(Train['SibSp'])

sns.countplot(Train['Embarked'])

sns.histplot(data = Train, x = 'Age')

sns.histplot(data = Train, x = 'Fare')

"""* The number of non survivors is higher compared to those who survived

* There is high number of third class passengers compared to first and second class

* There is more number of males than females

* Sibling Spouse(sibsp) 0 is higher compared to the rest

* Most passengers emberked from Southampton(S) compared to Cherbourg and Queenstown

#Bivariate Analysis(Train Dataset)
"""

sns.countplot(Train['Sex'], hue = Train['Survived'])

sns.countplot(Train['Pclass'], hue = Train['Survived'])

sns.countplot(Train['SibSp'], hue = Train['Survived'])

sns.countplot(Train['Embarked'], hue = Train['Survived'])

plt.figure(figsize = (7,5))
sns.histplot(data = Train, x = 'Age', hue = 'Survived')

sns.histplot(data = Train, x = 'Fare', hue = 'Survived')

"""* There is a higher number of male non survivors than females

* There is a higher number of third class passengers who did not survive compared to other classes

* Most passengers who did not survive embarked from southampton

* Most Sibling Spouse(SibSp) 0 did not survive compared to the rest

#Exploratory Data Analysis(Train Dataset)

Encoding Categorical data
"""

#Performing feature engineering on our two categorical columns
Train['Sex'] = Train['Sex'].apply(lambda x:1 if x == 'female' else 0)

Train['Embarked'] = Train['Embarked'].map({'S':1,'Q':2,'C':3})

Train.head()

Train.dtypes

Train.drop(['Parch'], axis = 1, inplace=True)

Test.shape

Train.shape

"""#Checking for Multicollinearity"""

#Data pre-processing
X = Train.iloc[:, 1:].values
y = Train.iloc[:, 0].values

print(X)

print(y)

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

x = add_constant(Train)

pd.Series([variance_inflation_factor(x.values, i)
for i in range(x.shape[1])], index=x.columns)

"""* There is multicollinearity between variables in our dataset

#K Nearest Neighbor Classification
"""

#Splitting the dataset into training set and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Training K-NN model on the Training dataset
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

"""#Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""*With K Nearest Neighbor we've managed to achieve an accuracy of 81% using default parameters


"""

Train.shape

"""#Predicting K-NN on the 2nd Dataset(Test Dataset)"""

Test_subset = Test.sample(n = 142)

Test_subset.replace([np.inf, -np.inf], np.nan, inplace = True)

Test_subset = Test_subset.dropna(axis=0, how = 'any', thresh=None, subset=None, inplace=False)

Test_subset_ = Test_subset.values

np.any(np.isnan(Test_subset))

np.all(np.isfinite(Test_subset))

np.ma.masked_array(Test_subset, ~np.isfinite(Test_subset)).filled(0)

X2 = Test_subset[['Pclass','Age','SibSp','Parch','Fare','Embarked']].values
y2 = Test_subset['Sex'].values.reshape(-1, 1)

print(y2)

print(X2)

y_pred = classifier.predict(X2)

print(accuracy_score(y2, y_pred))

"""* we've got a low accuracy of 25% after training our dataset using knn and tested it on a different similar dataset that lacked the target variable"""